\chapter{Feature Extraction}
\label{chap:Feature Extraction}
Feature extraction is a method of enhancing the performance of classification algorithms. Through different algorithms data sets with a large feature space is reduced to a smaller feature space. Classification algorithms has fewer features to assess, but the information gain of each feature is significantly larger, depending on the application. For anomaly detection, feature extraction focuses on producing features that specifically divides anomalies from normal data. Feature extraction can also be implemented for the isolation of different failures, this however is not within the scope of this thesis and only feature extraction for anomalies is discussed.

%\subsection{Linear Regression}

\section{Local Outlier Factor}
\label{section:OutlierFactor}
Most algorithms for anomaly detection are based on a metric which accounts for the entire dataset~\cite{breunig2000lof}. However, many anomalies are identifiable in relation to the local neighbourhood of data points and not the overall dataset. Therefore, \cite{breunig2000lof} developed the local outlier factor (LOF) algorithm that provides a measure of a data point's "outlierness" within a subset of data points. This implies that a data point is not classified as an anomaly or not, but a local outlier factor is calculated to determine how much a data point is distantiated from it's $k$-nearest neighbours. This is clearly demonstrated in Figure~\ref{fig:localOutlierFactor} where the data points which are clustered together have smaller outlier scores than data points which are removed from the highly dense areas. The radious of the red circle around each data point is equivalent to the outlier score, $\mathbf{LOF}$.

\begin{figure}[!hbt]
	\centering
	\import{Figures/}{LOF.pgf}
	\caption{Local outlier factor of random data set to demonstrate the outlier score produced by the local outlier factor algorithm.}
	\label{fig:localOutlierFactor}
\end{figure}

To calculate the $\mathbf{LOF}$, the $k$-distance, $d_k$, must be calculated and also the local reachability density $\mathbf{LRD}$. The $d_k$, is the $k^{th}$ ranked $d(p_o,p_i)$. Where $d(p_o,p_i)$ is the distance between data point $p_o$ and any data point $p_i$, with $i \in N$, where $N$ is the number of data points within the dataset with a minimum value of $N_{min}$. The minimum data points for a data point, $p_o$, is a function of the minimum value of points and denoted as $N_{min}(d_o)$. To reduce fluctuations in the $d(p_o,p_i)$ the distance between $p_o$ and $p_i$ is replaced with 
\begin{equation}
\textbf{max} \left( d(p_o,p_i), d_k \right),
\end{equation}
and will henceforth be referred to as the reachability distance, $\mathbf{d}$, ~\cite{breunig2000lof}. The $\mathbf{LRD}$ of a data point, $p_o$, is calculated as 
\begin{equation}
\mathbf{LRD}_{N_{min}}(p_o) = 1/\left(\frac{\sum\limits_{p_i \in N_{min}(p_o)}^{} \mathbf{d}_{N_{min}}(p_o,p_i)}{\left|N_{min}(p_o) \right|}\right),
\label{Eq-lrd}
\end{equation}
and denotes "the inverse of the average reachability distance based on the $N_{min}$-nearest neighbours of the $p_o$"~\cite{breunig2000lof}. Eq~\ref{Eq-lrd} enables the calculation for the $\mathbf{LOF}$ of point $p_o$ given as
\begin{equation}
\mathbf{LOF}_{N_{min}}(p_o) = \frac{\sum\limits_{p_i \in N_{min}(p_o)}^{}\frac{\mathbf{LRD}_{N_{min}}(p_i)}{\mathbf{LRD}_{N_{min}}(p_o)}}{\left| N_{min}(p_o) \right|}.
\label{Eq-LOF}
\end{equation}
The rule of thumb for detecting an outlier is that when the LOF is larger than $1$, then the point is considered an outlier with respect to its neighbourhood. This however is not fixed and the threshold can be changed depending on the application. LOF can therefore also be a anomaly detection algorithm if a given threshold is implemented otherwise the $\mathbf{LOF}$ can be given as an additional feature for other anomaly detection algorithms.

This method is aimed at producing a measure of the "outlierness" of a data point within a local neighbourhood and not for all the data points. This method will thus be implemented for the satellite anomaly detection, since it will detect anomalies within the two neighbourhoods produced by the eclipse during orbit. This method will also be able to detect anomalies from measurements of the nadir sensor, sun sensor and magnetometers that drastically change from the training data set. 

\begin{figure}[!htb]
	\centering
	
	\import{Figures/TexFigures/FeatureExtraction-LOF/Predictor-None/Isolator-None/Recovery-None/EARTH_SUN-ORC-General CubeSat Model/Reflection/}{LOF.pgf}
	
	\caption{Local outlier factor during first 2 orbits. The red area is when the sun reflection anomaly occurs}
	\label{fig:reflectionLOF}
\end{figure}

\begin{figure}[!htb]
	\centering
	
	\import{Figures/TexFigures/FeatureExtraction-LOF/Predictor-None/Isolator-None/Recovery-None/EARTH_SUN-ORC-General CubeSat Model/solarPanelDipole/}{LOF.pgf}
	
	\caption{Local outlier factor during first 2 orbits. The red area is when the solar panel dipole anomaly occurs}
	\label{fig:solarPanelDipoleLOF}
\end{figure}


\section{Linear Regression Model}
The proposed method by \cite{DeSilva2020} uses Dynamic Mode Decomposition (DMD), which was initially developed by \cite{schmid2011applications} and further expanded to include control by \cite{proctor2016dynamic}, to provide an estimation of a sensor vector based on the previous measurement of the sensor as well as the measurements of the other sensors in the system. DMD was first developed in the fluids community and constructed a matrix $\mathbf{A}$ to relate the state vector $x$ with the following time step of the state vector, $x_{k+1}$. The state vector, in our case, will be the measurement vector of the specific sensor that we want to monitor.
\begin{equation}
	\mathbf{x}_{k+1} = \mathbf{Ax}_k
\end{equation}
Where $\mathbf{x}_k$ and $\mathbf{x}_{k+1}$ during a specified number of time steps, will be denoted as $\mathbf{X}$ and $\mathbf{X'}$ respectively.

The method of DMD, however, is useful for high order systems where the calculation of $\mathbf{A}$ is computationally intensive. This is not the case for our system, and using DMD is not justifiable and consequently, a linear regression model is implemented. Therefore with the pseudo-inverse of $\mathbf{X}$, denoted as $\mathbf{X^{\dagger}}$, we calculate $\mathbf{A}$ as
\begin{equation}
	\mathbf{A} = \mathbf{X}\mathbf{X^{\dagger}}
\end{equation}
This necessitates data of the state vector over time. The article by \cite{DeSilva2020} however includes $\mathbf{B}$ to relate the vector measurements of the other sensors to adjust the predicted state, $X_{k+1}$ of the monitored sensor. 
\begin{equation}
	\mathbf{X}_{k+1} = \mathbf{AX}_k + \mathbf{BY}_k
	\label{control DMD}
\end{equation}
Where $\mathbf{Y}_k$ is the other sensor measurements, this is adjusted for our use case, where $\mathbf{Y}_k$ is the control torques for the magnetorquers and reaction wheels, while $\mathbf{X}_k$ is all of the sensor measurements. Consequently, the model of Eq~\ref{control DMD} denotes the prediction of the sensor measurements at time step $k+1$ based on the current sensor measurements and control inputs.
Thereafter, as implemented by \cite{DeSilva2020} the model is adjusted with a Kalman Filter. From $\mathbf{A}$ and $\mathbf{B}$ the Kalman filter can be implemented to predict $\mathbf{X}_{k+1}$
\begin{equation}
	\hat{\mathbf{X}}_{k+1} = \mathbf{A}\hat{\mathbf{X}}_k + \mathbf{B}\mathbf{Y}_k + K(\mathbf{X}_k - \hat{\mathbf{X}}_k)
\end{equation}
where $K = 0.001$. After the calculation of $\hat{\mathbf{X}}_{k+1}$ \cite{DeSilva2020} proposes a moving average of the innovation covariance
\begin{equation}
	\mathbf{V}_k = \frac{1}{N} \sum_{i=k-N}^k (\mathbf{X}_i - \hat{\mathbf{X}}_i)(\mathbf{X}_i - \hat{\mathbf{X}}_i)^T
\end{equation}
where $N$ is the number of timesteps to account for. The moving average is used as an additional input parameter for the classification of anomalies based on $\mathbf{X}$.

\begin{figure}[!htb]
	\centering
	
	\import{Figures/TexFigures/FeatureExtraction-DMD/Predictor-None/Isolator-None/Recovery-None/EARTH_SUN-ORC-General CubeSat Model/Reflection/}{DMD.pgf}
	
	\caption{Summation of moving average during first 2 orbits. The red area is when the sun reflection anomaly occurs}
	\label{fig:reflectionDMD}
\end{figure}

\begin{figure}[!htb]
	\centering
	
	\import{Figures/TexFigures/FeatureExtraction-DMD/Predictor-None/Isolator-None/Recovery-None/EARTH_SUN-ORC-General CubeSat Model/solarPanelDipole/}{DMD.pgf}
	
	\caption{Summation of moving average during first 2 orbits. The red area is when the solar panel dipole anomaly occurs}
	\label{fig:solarPanelDipoleDMD}
\end{figure}


\section{Summary}
The feature extraction methods discussed in this chapter focuses on extracting features that provide information gain on whether a data point is an anomaly or not. For the LOF algorithm this is done by providing a value for the "outlierness" of that data point relative to its nearest neighbourhood~\cite{breunig2000lof}. The linear regression model is an adaptation of work by \cite{DeSilva2020} to provides an estimate of the sensor measurements based on the previous sensor measurements as well as the control inputs. This can be extended to calculate a moving average of the estimated sensor measurements and the actual sensor measurements.